{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchmetrics import Accuracy, CohenKappa\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, ShuffleSplit\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scarce_dataset(image_list, n_samples=150):\n",
    "    samples =  np.random.choice(image_list, n_samples, replace=False)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_key = {\n",
    "    \"KL01\": 0,\n",
    "    \"KL234\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, images, label_key, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = label_key\n",
    "        self.transforms = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        label = self.labels[img.split(\"/\")[-2]]\n",
    "        img = np.array(Image.open(img).convert(\"RGB\"))\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(image=img)[\"image\"]\n",
    "            \n",
    "        return img.float(), torch.tensor(label).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_arch=\"VGG\"):\n",
    "    import copy\n",
    "    if model_arch == \"VGG\":\n",
    "        model = copy.deepcopy(models.vgg11_bn(weights=models.VGG11_BN_Weights.DEFAULT))\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_ = False    \n",
    "        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 1)\n",
    "    else:\n",
    "        model = copy.deepcopy(models.resnet18(weights=models.ResNet18_Weights.DEFAULT))\n",
    "        # for param in model.parameters():\n",
    "        #     param.requires_grad_ = False\n",
    "        model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "epochs = 10\n",
    "lr = 1e-05\n",
    "KL01_real = np.array(list(glob.iglob(\"/data_vault/hexai/KL01_KL234_Real/KL01/**\")))\n",
    "KL234_real = np.array(list(glob.iglob(\"/data_vault/hexai/KL01_KL234_Real/KL234/**\")))\n",
    "\n",
    "KL = create_scarce_dataset(np.concatenate([KL01_real, KL234_real]), n_samples=n_samples)\n",
    "#KL234 = create_scarce_dataset(KL234_real, n_samples=n_samples)\n",
    "#all_real = np.concatenate([KL01, KL234])\n",
    "\n",
    "# KL01_fake = np.array(list(glob.iglob(\"/data_vault/hexai/SyntheticKneeImages/KL01/**\")))\n",
    "# KL234_fake = np.array(list(glob.iglob(\"/data_vault/hexai/SyntheticKneeImages/KL234/**\")))\n",
    "# all_fake =  np.concatenate([KL01_fake, KL234_fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test  = train_test_split(KL, random_state=42, test_size=0.5, shuffle=True)\n",
    "train_y_strat = [labels_key[img.split(\"/\")[-2]] for img in train]\n",
    "augmentations = A.Compose([A.Resize(224, 224), ToTensorV2()])\n",
    "\n",
    "\n",
    "test_dataset = ClassificationDataset(test, labels_key, transform=augmentations)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 on REAL Images ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "criterion = nn.BCELoss()\n",
    "cohen_kappa = CohenKappa(task=\"binary\").to(device)\n",
    "model=\"resnet18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "test_size=0.4\n",
    "kfold_cv = ShuffleSplit(n_splits=5, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = int(len(train) * test_size)\n",
    "train_size = len(train) - valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in kfold_cv.split(train):\n",
    "    print(len(split[0]), len(split[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size, valid_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas = []\n",
    "best_models = []\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold_cv.split(train)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Define datasets and dataloaders for training and validation\n",
    "    train_dataset = ClassificationDataset(train[train_idx], labels_key, transform=augmentations)\n",
    "    valid_dataset = ClassificationDataset(train[valid_idx], labels_key, transform=augmentations)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    valid_kappas = []\n",
    "    best_model = None\n",
    "    best_kappa = -1.\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(model).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        running_kappa = []\n",
    "        \n",
    "        # Training\n",
    "        for img, label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float()\n",
    "            out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(running_loss)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_kappa = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, label in valid_dataloader:\n",
    "                img = img.to(device)\n",
    "                label = label.to(device).float()\n",
    "                out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "                out = outputs.extend([1 if o.cpu().item() > 0.5 else 0 for o in out])\n",
    "                labels.extend(label.cpu())\n",
    "                \n",
    "        \n",
    "        val_kappa = cohen_kappa_score(labels, outputs)\n",
    "        valid_kappas.append(val_kappa)\n",
    "        print(f\"Val. Kappa: {val_kappa}\")\n",
    "        \n",
    "        # Save best model based on validation kappa\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate best model on validation set outside of training loop\n",
    "    best_model.eval()\n",
    "    best_models.append(best_model)\n",
    "    print(f\"Best Validation Kappa: {best_kappa}\")\n",
    "    kfold_kappas.append(best_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "\n",
    "# Calculate the mean and standard deviation of the scores\n",
    "mean_score = np.mean(kfold_kappas)\n",
    "std_score = np.std(kfold_kappas)\n",
    "sem_score = std_score/np.sqrt(len(kfold_kappas))\n",
    "\n",
    "# Calculate the 95% confidence interval using the t-distribution\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(kfold_kappas) - 1\n",
    "confidence_interval = st.t.interval(confidence_level, degrees_freedom, mean_score, sem_score)\n",
    "\n",
    "print(f\"Mean Score: {mean_score}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold=np.argmax(kfold_kappas)\n",
    "best_model = best_models[best_fold]\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        out = torch.sigmoid(best_model(img)).squeeze(dim=-1)\n",
    "        outputs.extend([1 if o.item() > 0.5 else 0 for o in out])\n",
    "        labels.extend(label)\n",
    "\n",
    "print(f\"Final Test Kappa: {cohen_kappa_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 on REAL Images + 50% FAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_fake_images = int(train_size * .5 + train_size + train_size*1.5 + train_size * 2) - train_size\n",
    "KL01_fake = np.array(list(glob.iglob(\"/data_vault/hexai/SyntheticKneeImages/KL01/**\")))\n",
    "KL234_fake = np.array(list(glob.iglob(\"/data_vault/hexai/SyntheticKneeImages/KL234/**\")))\n",
    "KL01_fake_aug = create_scarce_dataset(KL01_fake, n_samples=total_fake_images)\n",
    "KL234_fake_aug = create_scarce_dataset(KL234_fake, n_samples=total_fake_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = int(train_size * 0.5)\n",
    "all_fake = np.concatenate([KL01_fake_aug[:n_fake], KL234_fake_aug[:n_fake]])\n",
    "np.random.shuffle(all_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kfold_kappas = []\n",
    "best_models = []\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold_cv.split(train, train_y_strat)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Define datasets and dataloaders for training and validation\n",
    "    train_comb = np.concatenate([train[train_idx], all_fake])\n",
    "    np.random.shuffle(train_comb)\n",
    "    train_dataset = ClassificationDataset(train_comb, labels_key, transform=augmentations)\n",
    "    valid_dataset = ClassificationDataset(train[valid_idx], labels_key, transform=augmentations)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    valid_kappas = []\n",
    "    best_model = None\n",
    "    best_kappa = -1.\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(\"VGG\").to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        running_kappa = []\n",
    "        \n",
    "        # Training\n",
    "        for img, label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float()\n",
    "            out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(running_loss)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_kappa = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, label in valid_dataloader:\n",
    "                img = img.to(device)\n",
    "                label = label.to(device).float()\n",
    "                out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "                out = outputs.extend([1 if o.cpu().item() > 0.5 else 0 for o in out])\n",
    "                labels.extend(label.cpu())\n",
    "                \n",
    "        \n",
    "        val_kappa = cohen_kappa_score(labels, outputs)\n",
    "        valid_kappas.append(val_kappa)\n",
    "        print(f\"Val. Kappa: {val_kappa}\")\n",
    "        \n",
    "        # Save best model based on validation kappa\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate best model on validation set outside of training loop\n",
    "    best_model.eval()\n",
    "    best_models.append(best_model)\n",
    "    print(f\"Best Validation Kappa: {best_kappa}\")\n",
    "    kfold_kappas.append(best_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "\n",
    "# Calculate the mean and standard deviation of the scores\n",
    "mean_score = np.mean(kfold_kappas)\n",
    "std_score = np.std(kfold_kappas)\n",
    "sem_score = std_score/np.sqrt(len(kfold_kappas))\n",
    "\n",
    "# Calculate the 95% confidence interval using the t-distribution\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(kfold_kappas) - 1\n",
    "confidence_interval = st.t.interval(confidence_level, degrees_freedom, mean_score, sem_score)\n",
    "\n",
    "print(f\"Mean Score: {mean_score}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold=np.argmax(kfold_kappas)\n",
    "best_model = best_models[best_fold]\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        out = torch.sigmoid(best_model(img)).squeeze(dim=-1)\n",
    "        outputs.extend([1 if o.item() > 0.5 else 0 for o in out])\n",
    "        labels.extend(label)\n",
    "\n",
    "print(f\"Final Test Kappa: {cohen_kappa_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 on REAL Images + 100% FAKE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = int(train_size * 1.0)\n",
    "all_fake =  np.concatenate([KL01_fake_aug[:n_fake], KL234_fake_aug[:n_fake]])\n",
    "np.random.shuffle(all_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas = []\n",
    "best_models = []\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold_cv.split(train)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Define datasets and dataloaders for training and validation\n",
    "    train_comb = np.concatenate([train[train_idx], all_fake])\n",
    "    np.random.shuffle(train_comb)\n",
    "    train_dataset = ClassificationDataset(train_comb, labels_key, transform=augmentations)    \n",
    "    valid_dataset = ClassificationDataset(train[valid_idx], labels_key, transform=augmentations)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    valid_kappas = []\n",
    "    best_model = None\n",
    "    best_kappa = -1.\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(\"VGG\").to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        running_kappa = []\n",
    "        \n",
    "        # Training\n",
    "        for img, label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float()\n",
    "            out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(running_loss)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_kappa = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, label in valid_dataloader:\n",
    "                img = img.to(device)\n",
    "                label = label.to(device).float()\n",
    "                out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "                out = outputs.extend([1 if o.cpu().item() > 0.5 else 0 for o in out])\n",
    "                labels.extend(label.cpu())\n",
    "                \n",
    "        \n",
    "        val_kappa = cohen_kappa_score(labels, outputs)\n",
    "        valid_kappas.append(val_kappa)\n",
    "        print(f\"Val. Kappa: {val_kappa}\")\n",
    "        \n",
    "        # Save best model based on validation kappa\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate best model on validation set outside of training loop\n",
    "    best_model.eval()\n",
    "    best_models.append(best_model)\n",
    "    print(f\"Best Validation Kappa: {best_kappa}\")\n",
    "    kfold_kappas.append(best_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold=np.argmax(kfold_kappas)\n",
    "best_model = best_models[best_fold]\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        out = torch.sigmoid(best_model(img)).squeeze(dim=-1)\n",
    "        outputs.extend([1 if o.item() > 0.5 else 0 for o in out])\n",
    "        labels.extend(label)\n",
    "\n",
    "print(f\"Final Test Kappa: {cohen_kappa_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 on REAL Images + 150% FAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = int(train_size * 1.5)\n",
    "all_fake =  np.concatenate([KL01_fake_aug[:n_fake], KL234_fake_aug[:n_fake]])\n",
    "np.random.shuffle(all_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas = []\n",
    "best_models = []\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold_cv.split(train, train_y_strat)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Define datasets and dataloaders for training and validation\n",
    "    train_comb = np.concatenate([train[train_idx], all_fake])\n",
    "    np.random.shuffle(train_comb)\n",
    "    train_dataset = ClassificationDataset(train_comb, labels_key, transform=augmentations)    \n",
    "    valid_dataset = ClassificationDataset(train[valid_idx], labels_key, transform=augmentations)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    valid_kappas = []\n",
    "    best_model = None\n",
    "    best_kappa = -1.\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(\"VGG\").to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        running_kappa = []\n",
    "        \n",
    "        # Training\n",
    "        for img, label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float()\n",
    "            out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(running_loss)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_kappa = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, label in valid_dataloader:\n",
    "                img = img.to(device)\n",
    "                label = label.to(device).float()\n",
    "                out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "                out = outputs.extend([1 if o.cpu().item() > 0.5 else 0 for o in out])\n",
    "                labels.extend(label.cpu())\n",
    "                \n",
    "        \n",
    "        val_kappa = cohen_kappa_score(labels, outputs)\n",
    "        valid_kappas.append(val_kappa)\n",
    "        print(f\"Val. Kappa: {val_kappa}\")\n",
    "        \n",
    "        # Save best model based on validation kappa\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate best model on validation set outside of training loop\n",
    "    best_model.eval()\n",
    "    best_models.append(best_model)\n",
    "    print(f\"Best Validation Kappa: {best_kappa}\")\n",
    "    kfold_kappas.append(best_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st \n",
    "\n",
    "# Calculate the mean and standard deviation of the scores\n",
    "mean_score = np.mean(kfold_kappas)\n",
    "std_score = np.std(kfold_kappas)\n",
    "sem_score = std_score/np.sqrt(len(kfold_kappas))\n",
    "\n",
    "# Calculate the 95% confidence interval using the t-distribution\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(kfold_kappas) - 1\n",
    "confidence_interval = st.t.interval(confidence_level, degrees_freedom, mean_score, sem_score)\n",
    "\n",
    "print(f\"Mean Score: {mean_score}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold=np.argmax(kfold_kappas)\n",
    "best_model = best_models[best_fold]\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        out = torch.sigmoid(best_model(img)).squeeze(dim=-1)\n",
    "        outputs.extend([1 if o.item() > 0.5 else 0 for o in out])\n",
    "        labels.extend(label)\n",
    "\n",
    "print(f\"Final Test Kappa: {cohen_kappa_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 on REAL Images + 200% FAKE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fake = int(train_size * 2.0)\n",
    "all_fake =  np.concatenate([KL01_fake_aug[:n_fake], KL234_fake_aug[:n_fake]])\n",
    "np.random.shuffle(all_fake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas = []\n",
    "best_models = []\n",
    "for i, (train_idx, valid_idx) in enumerate(kfold_cv.split(train, train_y_strat)):\n",
    "    print(f\"Fold {i}\")\n",
    "    \n",
    "    # Define datasets and dataloaders for training and validation\n",
    "    train_comb = np.concatenate([train[train_idx], all_fake])\n",
    "    np.random.shuffle(train_comb)\n",
    "    train_dataset = ClassificationDataset(train_comb, labels_key, transform=augmentations)    \n",
    "    valid_dataset = ClassificationDataset(train[valid_idx], labels_key, transform=augmentations)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=2, shuffle=False)\n",
    "    \n",
    "    valid_kappas = []\n",
    "    best_model = None\n",
    "    best_kappa = -1.\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = get_model(\"VGG\").to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        running_kappa = []\n",
    "        \n",
    "        # Training\n",
    "        for img, label in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(device)\n",
    "            label = label.to(device).float()\n",
    "            out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "            loss = criterion(out, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(running_loss)\n",
    "        print(f\"Train Loss: {train_loss}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        running_kappa = []\n",
    "        outputs = []\n",
    "        labels = []\n",
    "        with torch.no_grad():\n",
    "            for img, label in valid_dataloader:\n",
    "                img = img.to(device)\n",
    "                label = label.to(device).float()\n",
    "                out = torch.sigmoid(model(img)).squeeze(dim=-1)\n",
    "                out = outputs.extend([1 if o.cpu().item() > 0.5 else 0 for o in out])\n",
    "                labels.extend(label.cpu())\n",
    "                \n",
    "        \n",
    "        val_kappa = cohen_kappa_score(labels, outputs)\n",
    "        valid_kappas.append(val_kappa)\n",
    "        print(f\"Val. Kappa: {val_kappa}\")\n",
    "        \n",
    "        # Save best model based on validation kappa\n",
    "        if val_kappa > best_kappa:\n",
    "            best_kappa = val_kappa\n",
    "            best_model = copy.deepcopy(model)\n",
    "    \n",
    "    # Evaluate best model on validation set outside of training loop\n",
    "    best_model.eval()\n",
    "    best_models.append(best_model)\n",
    "    print(f\"Best Validation Kappa: {best_kappa}\")\n",
    "    kfold_kappas.append(best_kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_kappas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy.stats as st \n",
    "\n",
    "# Calculate the mean and standard deviation of the scores\n",
    "mean_score = np.mean(kfold_kappas)\n",
    "std_score = np.std(kfold_kappas)\n",
    "sem_score = std_score/np.sqrt(len(kfold_kappas))\n",
    "\n",
    "# Calculate the 95% confidence interval using the t-distribution\n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(kfold_kappas) - 1\n",
    "confidence_interval = st.t.interval(confidence_level, degrees_freedom, mean_score, sem_score)\n",
    "\n",
    "print(f\"Mean Score: {mean_score}\")\n",
    "print(f\"95% Confidence Interval: {confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_fold=np.argmax(kfold_kappas)\n",
    "best_model = best_models[best_fold]\n",
    "outputs = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for img, label in test_dataloader:\n",
    "        img = img.to(device)\n",
    "        out = torch.sigmoid(best_model(img)).squeeze(dim=-1)\n",
    "        outputs.extend([1 if o.item() > 0.5 else 0 for o in out])\n",
    "        labels.extend(label)\n",
    "\n",
    "print(f\"Final Test Kappa: {cohen_kappa_score(labels, outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
